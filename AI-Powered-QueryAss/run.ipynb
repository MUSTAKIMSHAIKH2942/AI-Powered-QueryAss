{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\OneDrive\\Desktop\\AI-Powered Smart Query Assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"Salesforce/codet5p-220m\"\n",
    "\n",
    "# Load Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)  # âœ… Use Seq2Seq Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    // Create a new customer\n",
      "    var customer = new Customer();\n",
      "    customer.name = \"New Customer\";\n",
      "    customer.email = \"new-customer@gmail.com\";\n",
      "    customer.phone = \"1234567890\";\n",
      "    customer.address = \"New Street\";\n",
      "    customer.city = \"New City\";\n",
      "    customer.state = \"New State\";\n",
      "    customer.country = \"New Country\";\n",
      "    customer.phone_\n"
     ]
    }
   ],
   "source": [
    "# Input: Natural Language Query\n",
    "input_text = \"Retrieve the top 10 customers by total purchase amount\"\n",
    "\n",
    "# Tokenize Input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate SQL Query\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "sql_query = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL: /*\n",
      " * Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " * http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS\n"
     ]
    }
   ],
   "source": [
    "nl_query = \"Find all orders placed in the last week.\"\n",
    "input_text = f\"Translate to SQL: {nl_query}\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_length=128)  \n",
    "sql_query = tokenizer.decode(output_ids[0], skip_special_tokens=True)  \n",
    "\n",
    "print(\"Generated SQL:\", sql_query)  # Check if it's correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = f\"Translate to SQL: {item['input']}\"\n",
    "        output_text = item[\"output\"]\n",
    "\n",
    "        input_encodings = tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        output_encodings = tokenizer(\n",
    "            output_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = output_encodings.input_ids.squeeze()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in loss computation\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings.input_ids.squeeze(),\n",
    "            \"attention_mask\": input_encodings.attention_mask.squeeze(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "sample_data = [\n",
    "    {\n",
    "        \"input\": \"Retrieve the top 10 customers by total purchase amount.\",\n",
    "        \"output\": \"SELECT customer_name, SUM(purchase_amount) AS total FROM orders GROUP BY customer_name ORDER BY total DESC LIMIT 10;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Get the total number of products in the inventory.\",\n",
    "        \"output\": \"SELECT COUNT(*) FROM products;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find employees who joined after 2020.\",\n",
    "        \"output\": \"SELECT * FROM employees WHERE join_date > '2020-01-01';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"List all customers who made purchases in the last 30 days.\",\n",
    "        \"output\": \"SELECT DISTINCT customer_name FROM orders WHERE order_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY);\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Retrieve all orders placed by a specific customer named John Doe.\",\n",
    "        \"output\": \"SELECT * FROM orders WHERE customer_name = 'John Doe';\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Get the average order value from the orders table.\",\n",
    "        \"output\": \"SELECT AVG(order_amount) FROM orders;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Show the 5 most recent transactions.\",\n",
    "        \"output\": \"SELECT * FROM transactions ORDER BY transaction_date DESC LIMIT 5;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Find the highest salary among all employees.\",\n",
    "        \"output\": \"SELECT MAX(salary) FROM employees;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"List the names and emails of all customers.\",\n",
    "        \"output\": \"SELECT name, email FROM customers;\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Retrieve the total revenue generated this month.\",\n",
    "        \"output\": \"SELECT SUM(amount) FROM transactions WHERE transaction_date >= DATE_FORMAT(CURDATE(), '%Y-%m-01');\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SQLDataset(sample_data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -U transformers accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 02:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1.3230331420898438, metrics={'train_runtime': 184.9332, 'train_samples_per_second': 0.27, 'train_steps_per_second': 0.081, 'total_flos': 7611973632000.0, 'train_loss': 1.3230331420898438, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sql_model_finetuned\",\n",
    "    evaluation_strategy=\"no\",  # <-- Change this to disable evaluation\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    # eval_dataset=eval_dataset  # <-- Add this\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Retrieve the top 10 customers by total purchase amount.\n",
      "Generated SQL: SELECT customer_name FROM orders WHERE order_date >= DATE_SUB(CURDATE(), SUB(CURDATE(), INTERVAL 10 DAY));\n",
      "\n",
      "Query: Get the total number of products in inventory.\n",
      "Generated SQL: SELECT COUNT(*) FROM products;\n",
      "\n",
      "Query: Find employees who joined after 2021.\n",
      "Generated SQL: \n",
      "SELECT * FROM employees WHERE joined_date >= DATE_SUB(CURDATE(), INTERVAL INTERVAL 30 DAY);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_sql(nl_query):\n",
    "    input_text = f\"Translate to SQL: {nl_query}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    output_ids = model.generate(input_ids, max_length=128)  \n",
    "    sql_query = tokenizer.decode(output_ids[0], skip_special_tokens=True)  \n",
    "\n",
    "    return sql_query\n",
    "\n",
    "# Sample Test\n",
    "test_queries = [\n",
    "    \"Retrieve the top 10 customers by total purchase amount.\",\n",
    "    \"Get the total number of products in inventory.\",\n",
    "    \"Find employees who joined after 2021.\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Generated SQL: {generate_sql(query)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.13-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.13-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.1-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.2 MB 2.0 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.2 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.0/25.2 MB 1.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 1.3/25.2 MB 1.3 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 1.6/25.2 MB 1.4 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 1.8/25.2 MB 1.3 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 2.4/25.2 MB 1.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 2.6/25.2 MB 1.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 3.1/25.2 MB 1.6 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 4.5/25.2 MB 1.7 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 5.0/25.2 MB 1.7 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 5.5/25.2 MB 1.8 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 5.8/25.2 MB 1.8 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 6.6/25.2 MB 1.9 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 1.9 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 7.6/25.2 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/25.2 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.7/25.2 MB 2.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 8.9/25.2 MB 2.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/25.2 MB 2.1 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.3/25.2 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.6/25.2 MB 2.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/25.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.7/25.2 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/25.2 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.1/25.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.1/25.2 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 20.2/25.2 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.2/25.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.5/25.2 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.9/25.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.5 MB 5.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/11.5 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.4/11.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp313-cp313-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.3.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp313-cp313-win_amd64.whl (315 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.3.0 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\musta\\onedrive\\desktop\\ai-powered smart query assistant\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.2M/26.2M [00:13<00:00, 1.88MB/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15878/15878 [00:02<00:00, 7079.98 examples/s] \n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8421/8421 [00:00<00:00, 8933.78 examples/s] \n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56355/56355 [00:06<00:00, 8783.75 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikisql\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikisql\", trust_remote_code=True)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Salesforce/codet5p-220m\"  # Change to another model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56355/56355 [00:05<00:00, 11189.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8421/8421 [00:00<00:00, 11691.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate SQL to English: {query}\" for query in examples[\"question\"]]\n",
    "    \n",
    "    # Extract \"human_readable\" text for each SQL query\n",
    "    targets = [sql[\"human_readable\"] for sql in examples[\"sql\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\OneDrive\\Desktop\\AI-Powered Smart Query Assistant\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./sql_to_natural_language_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musta\\AppData\\Local\\Temp\\ipykernel_38876\\334702887.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Data collator to pad sequences dynamically\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sql_to_natural_language_model\\\\tokenizer_config.json',\n",
       " './sql_to_natural_language_model\\\\special_tokens_map.json',\n",
       " './sql_to_natural_language_model\\\\vocab.json',\n",
       " './sql_to_natural_language_model\\\\merges.txt',\n",
       " './sql_to_natural_language_model\\\\added_tokens.json',\n",
       " './sql_to_natural_language_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./sql_to_natural_language_model\")\n",
    "tokenizer.save_pretrained(\"./sql_to_natural_language_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1053' max='1053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1053/1053 1:31:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.06804084777832, 'eval_model_preparation_time': 0.0068, 'eval_runtime': 5511.0255, 'eval_samples_per_second': 1.528, 'eval_steps_per_second': 0.191}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name FROM employees WHERE salary > 50000;\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./sql_to_natural_language_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Define the inference pipeline\n",
    "nlp_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test with a sample SQL query\n",
    "sql_query = \"SELECT name FROM employees WHERE salary > 50000;\"\n",
    "input_text = f\"translate SQL to English: {sql_query}\"\n",
    "\n",
    "# Generate output\n",
    "output = nlp_pipeline(input_text, max_length=128)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name FROM employees WHERE salary > 50000;\n"
     ]
    }
   ],
   "source": [
    "# Generate output\n",
    "output = nlp_pipeline(input_text, max_length=128)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT name FROM employees WHERE salary > 50000;\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./sql_to_natural_language_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Define the inference pipeline\n",
    "nlp_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0 if device.type == \"cuda\" else -1)\n",
    "\n",
    "# Test with a sample SQL query\n",
    "sql_query = \"SELECT name FROM employees WHERE salary > 50000;\"\n",
    "input_text = f\"translate SQL to English: {sql_query}\"\n",
    "\n",
    "# Generate output\n",
    "output = nlp_pipeline(input_text, max_length=128)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# checkpoint = \"Salesforce/codet5p-220m\"\n",
    "# device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# inputs = tokenizer.encode(\"def print_hello_world():<extra_id_0>\", return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, max_length=10)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# # ==> print \"Hello World\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Move inputs to GPU\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: False\n",
      "GPU name: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 14 13:07:06 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P0              4W /   42W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\Users\\musta\\OneDrive\\Desktop\\AI-Powered Smart Query Assistant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
